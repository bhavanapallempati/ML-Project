{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bhavana_GAN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChtcZvGRD6s_"
      },
      "source": [
        "\r\n",
        "from keras.datasets import mnist\r\n",
        "from keras.utils import np_utils\r\n",
        "from keras.models import Sequential, Model\r\n",
        "from keras.layers import Input, Dense, Dropout, Activation, Flatten\r\n",
        "from keras.layers.advanced_activations import LeakyReLU\r\n",
        "from keras.optimizers import Adam, RMSprop\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import random\r\n",
        "from tqdm import tqdm_notebook\r\n",
        "\r\n",
        "# Dataset of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images.\r\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkizudl9D_tj"
      },
      "source": [
        "X_train = X_train.reshape(60000, 784)\r\n",
        "X_test = X_test.reshape(10000, 784)\r\n",
        "X_train = X_train.astype('float32')/255\r\n",
        "X_test = X_test.astype('float32')/255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMeYXrOEEBaq"
      },
      "source": [
        "# Set the dimensions of the noise\r\n",
        "z_dim = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTtO9mPNEENo"
      },
      "source": [
        "# Optimizer\r\n",
        "adam = Adam(lr=0.0002, beta_1=0.5)\r\n",
        "\r\n",
        "g = Sequential()\r\n",
        "g.add(Dense(256, input_dim=z_dim, activation=LeakyReLU(alpha=0.2)))\r\n",
        "g.add(Dense(512, activation=LeakyReLU(alpha=0.2)))\r\n",
        "g.add(Dense(1024, activation=LeakyReLU(alpha=0.2)))\r\n",
        "g.add(Dense(784, activation='sigmoid'))  # Values between 0 and 1\r\n",
        "g.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\r\n",
        "\r\n",
        "d = Sequential()\r\n",
        "d.add(Dense(1024, input_dim=784, activation=LeakyReLU(alpha=0.2)))\r\n",
        "d.add(Dropout(0.3))\r\n",
        "d.add(Dense(512, activation=LeakyReLU(alpha=0.2)))\r\n",
        "d.add(Dropout(0.3))\r\n",
        "d.add(Dense(256, activation=LeakyReLU(alpha=0.2)))\r\n",
        "d.add(Dropout(0.3))\r\n",
        "d.add(Dense(1, activation='sigmoid'))  # Values between 0 and 1\r\n",
        "d.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\r\n",
        "\r\n",
        "d.trainable = False\r\n",
        "inputs = Input(shape=(z_dim, ))\r\n",
        "hidden = g(inputs)\r\n",
        "output = d(hidden)\r\n",
        "gan = Model(inputs, output)\r\n",
        "gan.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWaQ78t_EHa8"
      },
      "source": [
        "def plot_loss(losses):\r\n",
        "    \"\"\"\r\n",
        "    @losses.keys():\r\n",
        "        0: loss\r\n",
        "        1: accuracy\r\n",
        "    \"\"\"\r\n",
        "    d_loss = [v[0] for v in losses[\"D\"]]\r\n",
        "    g_loss = [v[0] for v in losses[\"G\"]]\r\n",
        "    #d_acc = [v[1] for v in losses[\"D\"]]\r\n",
        "    #g_acc = [v[1] for v in losses[\"G\"]]\r\n",
        "    \r\n",
        "    plt.figure(figsize=(10,8))\r\n",
        "    plt.plot(d_loss, label=\"Discriminator loss\")\r\n",
        "    plt.plot(g_loss, label=\"Generator loss\")\r\n",
        "    #plt.plot(d_acc, label=\"Discriminator accuracy\")\r\n",
        "    #plt.plot(g_acc, label=\"Generator accuracy\")\r\n",
        "    plt.xlabel('Epochs')\r\n",
        "    plt.ylabel('Loss')\r\n",
        "    plt.legend()\r\n",
        "    plt.show()\r\n",
        "    \r\n",
        "def plot_generated(n_ex=10, dim=(1, 10), figsize=(12, 2)):\r\n",
        "    noise = np.random.normal(0, 1, size=(n_ex, z_dim))\r\n",
        "    generated_images = g.predict(noise)\r\n",
        "    generated_images = generated_images.reshape(n_ex, 28, 28)\r\n",
        "\r\n",
        "    plt.figure(figsize=figsize)\r\n",
        "    for i in range(generated_images.shape[0]):\r\n",
        "        plt.subplot(dim[0], dim[1], i+1)\r\n",
        "        plt.imshow(generated_images[i], interpolation='nearest', cmap='gray_r')\r\n",
        "        plt.axis('off')\r\n",
        "    plt.tight_layout()\r\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_SbSZjIEK1V"
      },
      "source": [
        "losses = {\"D\":[], \"G\":[]}\r\n",
        "\r\n",
        "def train(epochs=1, plt_frq=1, BATCH_SIZE=128):\r\n",
        "    batchCount = int(X_train.shape[0] / BATCH_SIZE)\r\n",
        "    print('Epochs:', epochs)\r\n",
        "    print('Batch size:', BATCH_SIZE)\r\n",
        "    print('Batches per epoch:', batchCount)\r\n",
        "    \r\n",
        "    for e in tqdm_notebook(range(1, epochs+1)):\r\n",
        "        if e == 1 or e%plt_frq == 0:\r\n",
        "            print('-'*15, 'Epoch %d' % e, '-'*15)\r\n",
        "        for _ in range(batchCount):  # tqdm_notebook(range(batchCount), leave=False):\r\n",
        "            # Create a batch by drawing random index numbers from the training set\r\n",
        "            image_batch = X_train[np.random.randint(0, X_train.shape[0], size=BATCH_SIZE)]\r\n",
        "            # Create noise vectors for the generator\r\n",
        "            noise = np.random.normal(0, 1, size=(BATCH_SIZE, z_dim))\r\n",
        "            \r\n",
        "            # Generate the images from the noise\r\n",
        "            generated_images = g.predict(noise)\r\n",
        "            X = np.concatenate((image_batch, generated_images))\r\n",
        "            # Create labels\r\n",
        "            y = np.zeros(2*BATCH_SIZE)\r\n",
        "            y[:BATCH_SIZE] = 0.9  # One-sided label smoothing\r\n",
        "\r\n",
        "            # Train discriminator on generated images\r\n",
        "            d.trainable = True\r\n",
        "            d_loss = d.train_on_batch(X, y)\r\n",
        "\r\n",
        "            # Train generator\r\n",
        "            noise = np.random.normal(0, 1, size=(BATCH_SIZE, z_dim))\r\n",
        "            y2 = np.ones(BATCH_SIZE)\r\n",
        "            d.trainable = False\r\n",
        "            g_loss = gan.train_on_batch(noise, y2)\r\n",
        "\r\n",
        "        # Only store losses from final batch of epoch\r\n",
        "        losses[\"D\"].append(d_loss)\r\n",
        "        losses[\"G\"].append(g_loss)\r\n",
        "\r\n",
        "        # Update the plots\r\n",
        "        if e == 1 or e%plt_frq == 0:\r\n",
        "            plot_generated()\r\n",
        "    plot_loss(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGkXl535EM1a"
      },
      "source": [
        "train(epochs=200, plt_frq=20, BATCH_SIZE=128)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}